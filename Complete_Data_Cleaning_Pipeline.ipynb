{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be720264",
   "metadata": {},
   "source": [
    "# ğŸ¥ Complete Medical Imaging Data Cleaning Pipeline\n",
    "\n",
    "================================================================================\n",
    "\n",
    "This notebook performs a **comprehensive data cleaning pipeline** for a medical imaging dataset.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "### **PHASE 1: Initial Cleaning** (Creates clean_entry.csv & flagged.csv)\n",
    "- Download and load all CSV files from Google Drive\n",
    "- Clean data_entry.csv (primary source of truth)\n",
    "- Clean bounding box tables (bbox_2.csv, corrupted bbox)\n",
    "- Salvage data from corrupted data_entry\n",
    "- Combine and save initial cleaned data\n",
    "\n",
    "### **PHASE 2: Full Cleaning** (Creates final_clean.csv)\n",
    "- **STAGE 1:** Load clean_entry.csv and flagged.csv\n",
    "- **STAGE 2:** Analyze flagged data quality issues\n",
    "- **STAGE 3:** Clean flagged data â†’ cleaned2.0.csv\n",
    "- **STAGE 4:** Extract completely clean data â†’ cleaned3.0.csv  \n",
    "- **STAGE 5:** Merge all clean data â†’ final_clean.csv\n",
    "\n",
    "## Output Files\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `clean_entry.csv` | Initially cleaned data with bounding boxes |\n",
    "| `flagged.csv` | Data with quality issues for review |\n",
    "| `cleaned2.0.csv` | Flagged data with standardized values |\n",
    "| `cleaned3.0.csv` | Completely clean flagged data (no NaN) |\n",
    "| `final_clean.csv` | **Final merged dataset ready for ML** |\n",
    "| `cleaning_report.txt` | Detailed statistics and summary |\n",
    "\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8d57b",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“¦ SECTION 1: Setup and Install Dependencies\n",
    "---\n",
    "Install required packages and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30528c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gdown -q\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")\n",
    "print(f\"ğŸ“… Pipeline started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa951493",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ SECTION 2: Mount Google Drive and Create Directories\n",
    "---\n",
    "Mount Google Drive for data access and create output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "print(\"ğŸ”— Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/cleaned_data', exist_ok=True)\n",
    "os.makedirs('/content/flagged_data', exist_ok=True)\n",
    "os.makedirs('/content/logs', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive mounted!\")\n",
    "print(\"ğŸ“‚ Created directories:\")\n",
    "print(\"   - /content/cleaned_data\")\n",
    "print(\"   - /content/flagged_data\")\n",
    "print(\"   - /content/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7fd8f",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“¥ SECTION 3: Download CSV Files from Google Drive\n",
    "---\n",
    "Define download functions and download all 5 CSV files needed for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aebff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file links from Google Drive\n",
    "csv_links = [\n",
    "    (\"https://drive.google.com/file/d/1H6UARor5aXx4_tHACWxE3d4sBY5d3pJF/view?usp=drive_link\", \"bbox_2.csv\"),\n",
    "    (\"https://drive.google.com/file/d/15xqxut8ri1bGc8nMJi7dxv89Xf9QU3BO/view?usp=drive_link\", \"data_entry_.csv\"),\n",
    "    (\"https://drive.google.com/file/d/1KwggxzXcv7DQSH4JN4cTsXooCU1Rehz0/view?usp=drive_link\", \"data_entry.csv\"),\n",
    "    (\"https://drive.google.com/file/d/15RZxzpxgK-V8jYMWb8kzlfJm012KbOxP/view?usp=drive_link\", \"HACKATHON_CORRUPTED_bbox_list.csv\"),\n",
    "    (\"https://drive.google.com/file/d/1Q32l5pH-Fu0JSe8EbVCDIe5xES2Fmo1s/view?usp=drive_link\", \"HACKATHON_CORRUPTED_data_entry.csv\"),\n",
    "]\n",
    "\n",
    "def extract_file_id(link):\n",
    "    \"\"\"Extract Google Drive file ID from shareable link\"\"\"\n",
    "    if '/file/d/' in link:\n",
    "        return link.split('/file/d/')[1].split('/')[0]\n",
    "    return link\n",
    "\n",
    "def download_file(link, output_name):\n",
    "    \"\"\"Download a file from Google Drive\"\"\"\n",
    "    file_id = extract_file_id(link)\n",
    "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    print(f\"â¬‡ï¸  Downloading {output_name}...\")\n",
    "    gdown.download(download_url, output_name, quiet=False)\n",
    "    print(f\"âœ… Downloaded {output_name}\")\n",
    "\n",
    "# Download all CSV files\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“¥ DOWNLOADING CSV FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for link, filename in csv_links:\n",
    "    try:\n",
    "        download_file(link, f'/content/{filename}')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {filename}: {e}\")\n",
    "\n",
    "print(\"\\nâœ… All downloads complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b26468",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“Š SECTION 4: Load All CSV Files\n",
    "---\n",
    "Load all downloaded CSV files into pandas DataFrames and inspect their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š LOADING CSV FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load main data_entry.csv\n",
    "data_entry = pd.read_csv('/content/data_entry.csv')\n",
    "print(f\"âœ… Loaded data_entry.csv: {data_entry.shape}\")\n",
    "print(f\"   Columns: {list(data_entry.columns)}\")\n",
    "\n",
    "# Load small data_entry_.csv (optional)\n",
    "try:\n",
    "    data_entry_small = pd.read_csv('/content/data_entry_.csv')\n",
    "    print(f\"âœ… Loaded data_entry_.csv (small): {data_entry_small.shape}\")\n",
    "except:\n",
    "    data_entry_small = None\n",
    "    print(\"âš ï¸  data_entry_.csv not available\")\n",
    "\n",
    "# Load bounding box data\n",
    "bbox_2 = pd.read_csv('/content/bbox_2.csv')\n",
    "print(f\"âœ… Loaded bbox_2.csv: {bbox_2.shape}\")\n",
    "print(f\"   Columns: {list(bbox_2.columns)}\")\n",
    "\n",
    "# Load corrupted bounding box data\n",
    "bbox_corrupted = pd.read_csv('/content/HACKATHON_CORRUPTED_bbox_list.csv')\n",
    "print(f\"âœ… Loaded HACKATHON_CORRUPTED_bbox_list.csv: {bbox_corrupted.shape}\")\n",
    "print(f\"   Columns: {list(bbox_corrupted.columns)}\")\n",
    "\n",
    "# Load corrupted data_entry\n",
    "data_entry_corrupted = pd.read_csv('/content/HACKATHON_CORRUPTED_data_entry.csv')\n",
    "print(f\"âœ… Loaded HACKATHON_CORRUPTED_data_entry.csv: {data_entry_corrupted.shape}\")\n",
    "print(f\"   Columns: {list(data_entry_corrupted.columns)}\")\n",
    "\n",
    "print(\"\\nâœ… All CSV files loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6564f",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”§ SECTION 5: Normalize Column Names\n",
    "---\n",
    "Standardize column names across different files to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0421eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df):\n",
    "    \"\"\"\n",
    "    Standardize column names across different files.\n",
    "    Maps variations like 'patient age', 'Patient Age', 'PatientAge' to standard format.\n",
    "    \"\"\"\n",
    "    col_map = {\n",
    "        # Image Index variations\n",
    "        'image index': 'Image Index',\n",
    "        'imageindex': 'Image Index',\n",
    "        'image_index': 'Image Index',\n",
    "        \n",
    "        # Patient Age variations\n",
    "        'patient age': 'Patient Age',\n",
    "        'patientage': 'Patient Age',\n",
    "        'patient_age': 'Patient Age',\n",
    "        'age': 'Patient Age',\n",
    "        \n",
    "        # Patient Gender variations\n",
    "        'patient gender': 'Patient Gender',\n",
    "        'patientgender': 'Patient Gender',\n",
    "        'patient_gender': 'Patient Gender',\n",
    "        'gender': 'Patient Gender',\n",
    "        \n",
    "        # Finding Labels variations\n",
    "        'finding labels': 'Finding Labels',\n",
    "        'findinglabels': 'Finding Labels',\n",
    "        'finding_labels': 'Finding Labels',\n",
    "        'labels': 'Finding Labels',\n",
    "        \n",
    "        # Finding Label (singular - for bbox)\n",
    "        'finding label': 'Finding Label',\n",
    "        'findinglabel': 'Finding Label',\n",
    "        'finding_label': 'Finding Label',\n",
    "        'label': 'Finding Label',\n",
    "        \n",
    "        # Image dimensions\n",
    "        'originalimage[width': 'OriginalImage[Width',\n",
    "        'originalimage[height': 'OriginalImage[Height',\n",
    "        'originalimagewidth': 'OriginalImage[Width',\n",
    "        'originalimageheight': 'OriginalImage[Height',\n",
    "        'width': 'OriginalImage[Width',\n",
    "        'height': 'OriginalImage[Height',\n",
    "        \n",
    "        # Bbox coordinates\n",
    "        'bbox_x': 'x',\n",
    "        'bbox_y': 'y',\n",
    "        'bbox_w': 'w',\n",
    "        'bbox_h': 'h',\n",
    "        'bboxx': 'x',\n",
    "        'bboxy': 'y',\n",
    "        'bboxw': 'w',\n",
    "        'bboxh': 'h',\n",
    "    }\n",
    "    \n",
    "    # Create lowercase mapping\n",
    "    df.columns = df.columns.str.strip()\n",
    "    new_cols = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if col_lower in col_map:\n",
    "            new_cols[col] = col_map[col_lower]\n",
    "        else:\n",
    "            new_cols[col] = col\n",
    "    \n",
    "    df = df.rename(columns=new_cols)\n",
    "    return df\n",
    "\n",
    "# Apply normalization to all dataframes\n",
    "print(\"ğŸ”§ Normalizing column names...\")\n",
    "data_entry = normalize_columns(data_entry)\n",
    "bbox_2 = normalize_columns(bbox_2)\n",
    "bbox_corrupted = normalize_columns(bbox_corrupted)\n",
    "data_entry_corrupted = normalize_columns(data_entry_corrupted)\n",
    "\n",
    "print(\"âœ… Column normalization complete!\")\n",
    "print(f\"\\nğŸ“‹ data_entry columns: {list(data_entry.columns)}\")\n",
    "print(f\"ğŸ“‹ bbox_2 columns: {list(bbox_2.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801efb5e",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ§¹ SECTION 6: Clean Primary Data Entry (data_entry.csv)\n",
    "---\n",
    "Clean the main data_entry.csv file - the primary source of truth.\n",
    "\n",
    "**Cleaning steps:**\n",
    "1. Remove missing/fake images\n",
    "2. Clean Patient Age (convert formats, validate range)\n",
    "3. Validate image dimensions\n",
    "4. Validate Finding Labels (not empty)\n",
    "5. Validate Gender (M/F only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574121c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_entry_primary(df):\n",
    "    \"\"\"\n",
    "    Clean the main data_entry.csv - PRIMARY SOURCE OF TRUTH\n",
    "    \n",
    "    Checks performed:\n",
    "    1. Missing/Fake images (patterns: missing, mock, fake)\n",
    "    2. Invalid age formats (058Y â†’ 58, ?? â†’ NaN)\n",
    "    3. Unrealistic ages (< 0 or > 120)\n",
    "    4. Invalid image dimensions\n",
    "    5. Empty finding labels\n",
    "    6. Invalid gender values\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (clean_df, flagged_df)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial_count = len(df)\n",
    "    flagged_rows = []\n",
    "    \n",
    "    print(f\"\\nğŸ” Starting with {initial_count:,} rows\")\n",
    "    \n",
    "    # Issue 1: Missing/Fake images\n",
    "    print(\"\\n1ï¸âƒ£ Checking for missing/fake images...\")\n",
    "    missing_patterns = ['missing', 'Missing', 'mock', 'fake', 'Missing_file']\n",
    "    mask_missing = df['Image Index'].str.contains('|'.join(missing_patterns), case=False, na=False)\n",
    "    flagged = df[mask_missing].copy()\n",
    "    flagged['flag_reason'] = 'missing_or_fake_image'\n",
    "    flagged_rows.append(flagged)\n",
    "    df = df[~mask_missing]\n",
    "    print(f\"   ğŸš© Flagged: {mask_missing.sum():,} missing/fake images\")\n",
    "    \n",
    "    # Issue 2: Clean Patient Age\n",
    "    print(\"\\n2ï¸âƒ£ Cleaning Patient Age...\")\n",
    "    if 'Patient Age' in df.columns:\n",
    "        # Convert age format (058Y â†’ 58, ?? â†’ NaN)\n",
    "        df['Patient Age'] = df['Patient Age'].astype(str)\n",
    "        df['Patient Age'] = df['Patient Age'].str.replace('Y', '').str.replace('?', '').str.strip()\n",
    "        \n",
    "        # Flag invalid ages before conversion\n",
    "        mask_invalid_age = ~df['Patient Age'].str.match(r'^\\d+$', na=False)\n",
    "        flagged = df[mask_invalid_age].copy()\n",
    "        flagged['flag_reason'] = 'invalid_age_format'\n",
    "        flagged_rows.append(flagged)\n",
    "        \n",
    "        # Convert to numeric\n",
    "        df['Patient Age'] = pd.to_numeric(df['Patient Age'], errors='coerce')\n",
    "        \n",
    "        # Flag unrealistic ages\n",
    "        mask_unrealistic = (df['Patient Age'] < 0) | (df['Patient Age'] > 120) | df['Patient Age'].isna()\n",
    "        flagged = df[mask_unrealistic & ~mask_invalid_age].copy()\n",
    "        flagged['flag_reason'] = 'unrealistic_age'\n",
    "        flagged_rows.append(flagged)\n",
    "        \n",
    "        df = df[~mask_unrealistic]\n",
    "        print(f\"   ğŸš© Flagged: {(mask_invalid_age | mask_unrealistic).sum():,} invalid ages\")\n",
    "    \n",
    "    # Issue 3: Validate image dimensions\n",
    "    print(\"\\n3ï¸âƒ£ Validating image dimensions...\")\n",
    "    if 'OriginalImage[Width' in df.columns and 'OriginalImage[Height' in df.columns:\n",
    "        mask_invalid_dims = (\n",
    "            (df['OriginalImage[Width'] <= 0) | \n",
    "            (df['OriginalImage[Height'] <= 0) |\n",
    "            df['OriginalImage[Width'].isna() |\n",
    "            df['OriginalImage[Height'].isna()\n",
    "        )\n",
    "        flagged = df[mask_invalid_dims].copy()\n",
    "        flagged['flag_reason'] = 'invalid_dimensions'\n",
    "        flagged_rows.append(flagged)\n",
    "        df = df[~mask_invalid_dims]\n",
    "        print(f\"   ğŸš© Flagged: {mask_invalid_dims.sum():,} invalid dimensions\")\n",
    "    \n",
    "    # Issue 4: Validate Finding Labels (not empty)\n",
    "    print(\"\\n4ï¸âƒ£ Validating Finding Labels...\")\n",
    "    if 'Finding Labels' in df.columns:\n",
    "        mask_no_labels = df['Finding Labels'].isna() | (df['Finding Labels'].str.strip() == '')\n",
    "        flagged = df[mask_no_labels].copy()\n",
    "        flagged['flag_reason'] = 'no_finding_labels'\n",
    "        flagged_rows.append(flagged)\n",
    "        df = df[~mask_no_labels]\n",
    "        print(f\"   ğŸš© Flagged: {mask_no_labels.sum():,} rows with no labels\")\n",
    "        \n",
    "        # Clean up labels\n",
    "        df['Finding Labels'] = df['Finding Labels'].str.strip()\n",
    "    \n",
    "    # Issue 5: Validate Gender\n",
    "    print(\"\\n5ï¸âƒ£ Validating Gender...\")\n",
    "    if 'Patient Gender' in df.columns:\n",
    "        mask_invalid_gender = ~df['Patient Gender'].isin(['M', 'F'])\n",
    "        flagged = df[mask_invalid_gender].copy()\n",
    "        flagged['flag_reason'] = 'invalid_gender'\n",
    "        flagged_rows.append(flagged)\n",
    "        df = df[~mask_invalid_gender]\n",
    "        print(f\"   ğŸš© Flagged: {mask_invalid_gender.sum():,} invalid gender values\")\n",
    "    \n",
    "    # Combine all flagged rows\n",
    "    all_flagged = pd.concat(flagged_rows, ignore_index=True) if flagged_rows else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"âœ… CLEAN: {len(df):,} rows ({len(df)/initial_count*100:.1f}%)\")\n",
    "    print(f\"ğŸš© FLAGGED: {len(all_flagged):,} rows ({len(all_flagged)/initial_count*100:.1f}%)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return df, all_flagged\n",
    "\n",
    "# Run the cleaning function\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ§¹ PART A: CLEANING DATA_ENTRY (PRIMARY SOURCE OF TRUTH)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_entry_clean, data_entry_flagged = clean_data_entry_primary(data_entry)\n",
    "\n",
    "print(f\"\\nğŸ“Š Results:\")\n",
    "print(f\"   Clean records: {len(data_entry_clean):,}\")\n",
    "print(f\"   Flagged records: {len(data_entry_flagged):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db06c9",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“¦ SECTION 7: Clean Bounding Box Tables\n",
    "---\n",
    "Clean bounding box data from bbox_2.csv and corrupted bbox files.\n",
    "\n",
    "**Validation checks:**\n",
    "1. Missing/fake images\n",
    "2. Invalid box dimensions (9999, -1, negative)\n",
    "3. Invalid coordinates (negative x, y)\n",
    "4. Boxes outside image bounds\n",
    "5. Finding label mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af02986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bbox_table(bbox_df, valid_images_df, source_name):\n",
    "    \"\"\"\n",
    "    Clean bounding box data\n",
    "    \n",
    "    Args:\n",
    "        bbox_df: Bounding box DataFrame\n",
    "        valid_images_df: Valid images DataFrame for reference\n",
    "        source_name: Name of the source file for logging\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (clean_df, flagged_df)\n",
    "    \"\"\"\n",
    "    df = bbox_df.copy()\n",
    "    initial_count = len(df)\n",
    "    flagged_rows = []\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ {source_name}: Starting with {initial_count:,} boxes\")\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_cols = ['Image Index']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âš ï¸  Missing columns: {missing_cols}\")\n",
    "        print(f\"   Available columns: {list(df.columns)}\")\n",
    "        return pd.DataFrame(), df.copy()\n",
    "    \n",
    "    # Check what dimension columns we have\n",
    "    has_bbox_coords = all(col in df.columns for col in ['x', 'y', 'w', 'h'])\n",
    "    \n",
    "    # Create image metadata lookup\n",
    "    meta_cols = ['Image Index']\n",
    "    if 'OriginalImage[Width' in valid_images_df.columns:\n",
    "        meta_cols.append('OriginalImage[Width')\n",
    "    if 'OriginalImage[Height' in valid_images_df.columns:\n",
    "        meta_cols.append('OriginalImage[Height')\n",
    "    if 'Finding Labels' in valid_images_df.columns:\n",
    "        meta_cols.append('Finding Labels')\n",
    "    \n",
    "    image_meta = valid_images_df[meta_cols].set_index('Image Index').to_dict('index')\n",
    "    \n",
    "    # Issue 1: Missing/Fake images\n",
    "    print(f\"1ï¸âƒ£ Checking for missing/fake images...\")\n",
    "    missing_patterns = ['missing', 'Missing', 'mock', 'fake']\n",
    "    mask_missing = df['Image Index'].str.contains('|'.join(missing_patterns), case=False, na=False)\n",
    "    flagged = df[mask_missing].copy()\n",
    "    flagged['flag_reason'] = f'{source_name}_missing_image'\n",
    "    flagged_rows.append(flagged)\n",
    "    df = df[~mask_missing]\n",
    "    print(f\"   ğŸš© Flagged: {mask_missing.sum():,}\")\n",
    "    \n",
    "    # Issue 2: Invalid dimensions (only if columns exist)\n",
    "    if has_bbox_coords:\n",
    "        print(f\"2ï¸âƒ£ Checking for invalid box dimensions...\")\n",
    "        mask_invalid_dims = (\n",
    "            (df['w'] == 9999) | (df['w'] == -1) | (df['w'] <= 0) |\n",
    "            (df['h'] == 9999) | (df['h'] == -1) | (df['h'] <= 0) |\n",
    "            df['w'].isna() | df['h'].isna()\n",
    "        )\n",
    "        flagged = df[mask_invalid_dims].copy()\n",
    "        flagged['flag_reason'] = f'{source_name}_invalid_box_dimensions'\n",
    "        flagged_rows.append(flagged)\n",
    "        df = df[~mask_invalid_dims]\n",
    "        print(f\"   ğŸš© Flagged: {mask_invalid_dims.sum():,}\")\n",
    "        \n",
    "        # Issue 3: Invalid coordinates\n",
    "        print(f\"3ï¸âƒ£ Checking for invalid coordinates...\")\n",
    "        mask_invalid_coords = (df['x'] < 0) | (df['y'] < 0) | df['x'].isna() | df['y'].isna()\n",
    "        flagged = df[mask_invalid_coords].copy()\n",
    "        flagged['flag_reason'] = f'{source_name}_invalid_coordinates'\n",
    "        flagged_rows.append(flagged)\n",
    "        df = df[~mask_invalid_coords]\n",
    "        print(f\"   ğŸš© Flagged: {mask_invalid_coords.sum():,}\")\n",
    "        \n",
    "        # Issue 4: Boxes outside image bounds\n",
    "        if 'OriginalImage[Width' in meta_cols and 'OriginalImage[Height' in meta_cols:\n",
    "            print(f\"4ï¸âƒ£ Checking if boxes fit inside images...\")\n",
    "            out_of_bounds = []\n",
    "            valid_boxes = []\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                img_name = row['Image Index']\n",
    "                if img_name in image_meta:\n",
    "                    meta = image_meta[img_name]\n",
    "                    if 'OriginalImage[Width' in meta and 'OriginalImage[Height' in meta:\n",
    "                        img_width = meta['OriginalImage[Width']\n",
    "                        img_height = meta['OriginalImage[Height']\n",
    "                        \n",
    "                        if (row['x'] + row['w'] > img_width) or (row['y'] + row['h'] > img_height):\n",
    "                            out_of_bounds.append(idx)\n",
    "                        else:\n",
    "                            valid_boxes.append(idx)\n",
    "                    else:\n",
    "                        valid_boxes.append(idx)\n",
    "                else:\n",
    "                    out_of_bounds.append(idx)\n",
    "            \n",
    "            if out_of_bounds:\n",
    "                flagged = df.loc[out_of_bounds].copy()\n",
    "                flagged['flag_reason'] = f'{source_name}_box_out_of_bounds'\n",
    "                flagged_rows.append(flagged)\n",
    "            df = df.loc[valid_boxes] if valid_boxes else pd.DataFrame()\n",
    "            print(f\"   ğŸš© Flagged: {len(out_of_bounds):,}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Skipping bbox coordinate validation (columns not found)\")\n",
    "    \n",
    "    # Issue 5: Finding Label mismatch\n",
    "    if 'Finding Label' in df.columns and 'Finding Labels' in meta_cols:\n",
    "        print(f\"5ï¸âƒ£ Checking Finding Label consistency...\")\n",
    "        mismatch = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            img_name = row['Image Index']\n",
    "            if img_name in image_meta and 'Finding Labels' in image_meta[img_name]:\n",
    "                img_labels = image_meta[img_name]['Finding Labels']\n",
    "                box_label = row['Finding Label']\n",
    "                \n",
    "                if pd.notna(box_label) and pd.notna(img_labels):\n",
    "                    if box_label not in str(img_labels):\n",
    "                        mismatch.append(idx)\n",
    "                    else:\n",
    "                        valid_labels.append(idx)\n",
    "                else:\n",
    "                    valid_labels.append(idx)\n",
    "            else:\n",
    "                valid_labels.append(idx)\n",
    "        \n",
    "        if mismatch:\n",
    "            flagged = df.loc[mismatch].copy()\n",
    "            flagged['flag_reason'] = f'{source_name}_label_mismatch'\n",
    "            flagged_rows.append(flagged)\n",
    "        df = df.loc[valid_labels] if valid_labels else pd.DataFrame()\n",
    "        print(f\"   ğŸš© Flagged: {len(mismatch):,}\")\n",
    "    \n",
    "    # Combine all flagged\n",
    "    all_flagged = pd.concat(flagged_rows, ignore_index=True) if flagged_rows else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nâœ… CLEAN: {len(df):,} boxes ({len(df)/initial_count*100:.1f}%)\")\n",
    "    print(f\"ğŸš© FLAGGED: {len(all_flagged):,} boxes ({len(all_flagged)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    return df, all_flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c017ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean bbox_2.csv\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“¦ PART B: CLEANING BOUNDING BOXES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Cleaning bbox_2.csv\")\n",
    "print(\"-\" * 70)\n",
    "bbox_2_clean, bbox_2_flagged = clean_bbox_table(bbox_2, data_entry_clean, 'bbox_2')\n",
    "\n",
    "# Clean corrupted bbox\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Cleaning HACKATHON_CORRUPTED_bbox_list.csv\")\n",
    "print(\"-\" * 70)\n",
    "bbox_corrupted_clean, bbox_corrupted_flagged = clean_bbox_table(bbox_corrupted, data_entry_clean, 'bbox_corrupted')\n",
    "\n",
    "print(\"\\nğŸ“Š Bounding Box Cleaning Summary:\")\n",
    "print(f\"   bbox_2 clean: {len(bbox_2_clean):,}\")\n",
    "print(f\"   bbox_2 flagged: {len(bbox_2_flagged):,}\")\n",
    "print(f\"   bbox_corrupted clean: {len(bbox_corrupted_clean):,}\")\n",
    "print(f\"   bbox_corrupted flagged: {len(bbox_corrupted_flagged):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c8362",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”„ SECTION 8: Salvage Data from Corrupted Data Entry\n",
    "---\n",
    "Extract valid rows from the corrupted data_entry file that aren't already in the clean set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvage_corrupted_data_entry(corrupted_df, clean_df):\n",
    "    \"\"\"\n",
    "    Try to salvage good rows from corrupted data_entry\n",
    "    \n",
    "    Args:\n",
    "        corrupted_df: The corrupted data_entry DataFrame\n",
    "        clean_df: The already cleaned data_entry DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (salvaged_df, flagged_df)\n",
    "    \"\"\"\n",
    "    df = corrupted_df.copy()\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    print(f\"ğŸ” Starting with {initial_count:,} rows\")\n",
    "    \n",
    "    # Get images already in clean set\n",
    "    existing_images = set(clean_df['Image Index'].values)\n",
    "    \n",
    "    # Remove duplicates with clean set\n",
    "    mask_new = ~df['Image Index'].isin(existing_images)\n",
    "    df = df[mask_new]\n",
    "    print(f\"ğŸ”„ Removed {(~mask_new).sum():,} duplicates already in clean set\")\n",
    "    print(f\"ğŸ“ Processing {len(df):,} unique rows\")\n",
    "    \n",
    "    # Apply same cleaning rules\n",
    "    df_salvaged, df_flagged_salvage = clean_data_entry_primary(df)\n",
    "    \n",
    "    return df_salvaged, df_flagged_salvage\n",
    "\n",
    "# Salvage data from corrupted data_entry\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ PART C: SALVAGING DATA FROM CORRUPTED DATA_ENTRY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_entry_salvaged, data_entry_salvaged_flagged = salvage_corrupted_data_entry(\n",
    "    data_entry_corrupted, \n",
    "    data_entry_clean\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Salvage Results:\")\n",
    "print(f\"   âœ… SALVAGED: {len(data_entry_salvaged):,} additional clean rows\")\n",
    "print(f\"   ğŸš© FLAGGED: {len(data_entry_salvaged_flagged):,} flagged rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace6ac1",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”— SECTION 9: Combine Clean Data Entry with Bounding Boxes\n",
    "---\n",
    "Merge all clean data_entry sources with bounding box data using LEFT JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5860a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”— PART D: COMBINING CLEAN DATA_ENTRY + BBOX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Combine all clean data_entry sources\n",
    "all_clean_data_entry = pd.concat([data_entry_clean, data_entry_salvaged], ignore_index=True)\n",
    "all_clean_data_entry = all_clean_data_entry.drop_duplicates(subset=['Image Index'], keep='first')\n",
    "\n",
    "print(f\"ğŸ“Š Total clean data_entry records: {len(all_clean_data_entry):,}\")\n",
    "\n",
    "# Combine all clean bbox sources\n",
    "all_clean_bbox = pd.concat([bbox_2_clean, bbox_corrupted_clean], ignore_index=True)\n",
    "\n",
    "# Determine which columns to use for deduplication\n",
    "dedup_cols = ['Image Index']\n",
    "if 'Finding Label' in all_clean_bbox.columns:\n",
    "    dedup_cols.append('Finding Label')\n",
    "if 'x' in all_clean_bbox.columns:\n",
    "    dedup_cols.append('x')\n",
    "if 'y' in all_clean_bbox.columns:\n",
    "    dedup_cols.append('y')\n",
    "\n",
    "# Deduplicate\n",
    "if len(dedup_cols) > 1:\n",
    "    all_clean_bbox = all_clean_bbox.drop_duplicates(subset=dedup_cols, keep='first')\n",
    "    print(f\"ğŸ“Š Deduplicated bbox using columns: {dedup_cols}\")\n",
    "else:\n",
    "    all_clean_bbox = all_clean_bbox.drop_duplicates(subset=['Image Index'], keep='first')\n",
    "    print(f\"ğŸ“Š Deduplicated bbox using only: Image Index\")\n",
    "\n",
    "print(f\"ğŸ“Š Total clean bbox records: {len(all_clean_bbox):,}\")\n",
    "\n",
    "# Merge data_entry with bbox (LEFT JOIN)\n",
    "print(f\"\\nğŸ”— BBox columns available: {list(all_clean_bbox.columns)}\")\n",
    "\n",
    "if len(all_clean_bbox) > 0:\n",
    "    clean_entry_combined = all_clean_data_entry.merge(\n",
    "        all_clean_bbox,\n",
    "        on='Image Index',\n",
    "        how='left',\n",
    "        suffixes=('', '_bbox')\n",
    "    )\n",
    "    \n",
    "    # Clean up column names\n",
    "    if 'Finding Label' in clean_entry_combined.columns:\n",
    "        if 'Finding Labels' in clean_entry_combined.columns:\n",
    "            clean_entry_combined = clean_entry_combined.rename(columns={\n",
    "                'Finding Labels': 'diseases',\n",
    "                'Finding Label': 'bbox_disease'\n",
    "            })\n",
    "else:\n",
    "    clean_entry_combined = all_clean_data_entry.copy()\n",
    "    print(\"âš ï¸  No bbox data to merge\")\n",
    "\n",
    "print(f\"\\nâœ… CLEAN_ENTRY.CSV: {len(clean_entry_combined):,} records\")\n",
    "if 'bbox_disease' in clean_entry_combined.columns:\n",
    "    print(f\"   ğŸ“¦ With bounding boxes: {clean_entry_combined['bbox_disease'].notna().sum():,}\")\n",
    "    print(f\"   ğŸ“¦ Without bounding boxes: {clean_entry_combined['bbox_disease'].isna().sum():,}\")\n",
    "else:\n",
    "    print(f\"   ğŸ“¦ No bounding box data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf058b19",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸš© SECTION 10: Combine Flagged Data Entry with Bounding Boxes\n",
    "---\n",
    "Merge all flagged data sources using OUTER JOIN to capture all flagged records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸš© PART E: COMBINING FLAGGED DATA_ENTRY + BBOX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Combine all flagged data_entry sources\n",
    "all_flagged_data_entry = pd.concat([\n",
    "    data_entry_flagged, \n",
    "    data_entry_salvaged_flagged\n",
    "], ignore_index=True)\n",
    "all_flagged_data_entry = all_flagged_data_entry.drop_duplicates(subset=['Image Index'], keep='first')\n",
    "\n",
    "print(f\"ğŸ“Š Total flagged data_entry records: {len(all_flagged_data_entry):,}\")\n",
    "\n",
    "# Combine all flagged bbox sources\n",
    "all_flagged_bbox = pd.concat([bbox_2_flagged, bbox_corrupted_flagged], ignore_index=True)\n",
    "\n",
    "# Determine deduplication columns\n",
    "dedup_cols_flagged = ['Image Index']\n",
    "if 'Finding Label' in all_flagged_bbox.columns:\n",
    "    dedup_cols_flagged.append('Finding Label')\n",
    "if 'x' in all_flagged_bbox.columns:\n",
    "    dedup_cols_flagged.append('x')\n",
    "if 'y' in all_flagged_bbox.columns:\n",
    "    dedup_cols_flagged.append('y')\n",
    "\n",
    "# Deduplicate\n",
    "if len(dedup_cols_flagged) > 1:\n",
    "    all_flagged_bbox = all_flagged_bbox.drop_duplicates(subset=dedup_cols_flagged, keep='first')\n",
    "else:\n",
    "    all_flagged_bbox = all_flagged_bbox.drop_duplicates(subset=['Image Index'], keep='first')\n",
    "\n",
    "print(f\"ğŸ“Š Total flagged bbox records: {len(all_flagged_bbox):,}\")\n",
    "\n",
    "# Merge flagged data_entry with flagged bbox (OUTER JOIN)\n",
    "print(f\"\\nğŸ”— Flagged BBox columns available: {list(all_flagged_bbox.columns)}\")\n",
    "\n",
    "if len(all_flagged_bbox) > 0:\n",
    "    flagged_combined = all_flagged_data_entry.merge(\n",
    "        all_flagged_bbox,\n",
    "        on='Image Index',\n",
    "        how='outer',\n",
    "        suffixes=('', '_bbox')\n",
    "    )\n",
    "    \n",
    "    # Clean up column names\n",
    "    if 'Finding Label' in flagged_combined.columns:\n",
    "        if 'Finding Labels' in flagged_combined.columns:\n",
    "            flagged_combined = flagged_combined.rename(columns={\n",
    "                'Finding Labels': 'diseases',\n",
    "                'Finding Label': 'bbox_disease'\n",
    "            })\n",
    "else:\n",
    "    flagged_combined = all_flagged_data_entry.copy()\n",
    "    print(\"âš ï¸  No flagged bbox data to merge\")\n",
    "\n",
    "print(f\"\\nğŸš© FLAGGED.CSV: {len(flagged_combined):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6322c",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ’¾ SECTION 11: Save Initial Cleaning Results\n",
    "---\n",
    "Save clean_entry.csv and flagged.csv, generate summary statistics, and copy to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90934f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ’¾ PART F: SAVING INITIAL CLEANING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save clean_entry.csv\n",
    "clean_entry_combined.to_csv('/content/cleaned_data/clean_entry.csv', index=False)\n",
    "print(f\"âœ… Saved: clean_entry.csv ({len(clean_entry_combined):,} rows)\")\n",
    "\n",
    "# Save flagged.csv\n",
    "flagged_combined.to_csv('/content/flagged_data/flagged.csv', index=False)\n",
    "print(f\"ğŸš© Saved: flagged.csv ({len(flagged_combined):,} rows)\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'Total Original Records': len(data_entry) + len(data_entry_corrupted),\n",
    "    'Clean Records': len(clean_entry_combined),\n",
    "    'Flagged Records': len(flagged_combined),\n",
    "    'Clean Percentage': f\"{len(clean_entry_combined)/(len(data_entry) + len(data_entry_corrupted))*100:.2f}%\",\n",
    "}\n",
    "\n",
    "# Add bbox stats\n",
    "if 'bbox_disease' in clean_entry_combined.columns:\n",
    "    summary['Clean with BBox'] = int(clean_entry_combined['bbox_disease'].notna().sum())\n",
    "    summary['Clean without BBox'] = int(clean_entry_combined['bbox_disease'].isna().sum())\n",
    "else:\n",
    "    summary['Clean with BBox'] = 0\n",
    "    summary['Clean without BBox'] = len(clean_entry_combined)\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('/content/logs/cleaning_summary.csv', index=False)\n",
    "print(f\"ğŸ“Š Saved: cleaning_summary.csv\")\n",
    "\n",
    "# Save detailed flag reasons\n",
    "if 'flag_reason' in flagged_combined.columns:\n",
    "    flag_summary = flagged_combined['flag_reason'].value_counts().reset_index()\n",
    "    flag_summary.columns = ['Flag Reason', 'Count']\n",
    "    flag_summary.to_csv('/content/logs/flag_reasons.csv', index=False)\n",
    "    print(f\"ğŸ“‹ Saved: flag_reasons.csv\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Top 10 flag reasons:\")\n",
    "    print(flag_summary.head(10).to_string(index=False))\n",
    "\n",
    "# Copy to Google Drive\n",
    "drive_output = '/content/drive/MyDrive/xray_cleaned_dataset'\n",
    "os.makedirs(drive_output, exist_ok=True)\n",
    "\n",
    "shutil.copy('/content/cleaned_data/clean_entry.csv', f'{drive_output}/clean_entry.csv')\n",
    "shutil.copy('/content/flagged_data/flagged.csv', f'{drive_output}/flagged.csv')\n",
    "shutil.copytree('/content/logs', f'{drive_output}/logs', dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ… All files saved to Google Drive: {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d68016",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ‰ PHASE 1 COMPLETE: Initial Cleaning Summary\n",
    "---\n",
    "Initial cleaning is complete! We now have:\n",
    "- **clean_entry.csv**: Clean data with bounding boxes\n",
    "- **flagged.csv**: Data with quality issues for further processing\n",
    "\n",
    "Now we proceed to **PHASE 2: Full Cleaning** to further refine the flagged data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Cleaning Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ‰ PHASE 1 COMPLETE: INITIAL CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š INITIAL CLEANING STATISTICS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "âœ… CLEAN_ENTRY.CSV\n",
    "   Total records: {len(clean_entry_combined):,}\n",
    "   With bounding boxes: {clean_entry_combined['bbox_disease'].notna().sum() if 'bbox_disease' in clean_entry_combined.columns else 0:,}\n",
    "   Without bounding boxes: {clean_entry_combined['bbox_disease'].isna().sum() if 'bbox_disease' in clean_entry_combined.columns else len(clean_entry_combined):,}\n",
    "\n",
    "ğŸš© FLAGGED.CSV\n",
    "   Total records: {len(flagged_combined):,}\n",
    "   \n",
    "ğŸ“ˆ OVERALL\n",
    "   Clean rate: {len(clean_entry_combined)/(len(data_entry) + len(data_entry_corrupted))*100:.1f}%\n",
    "   \n",
    "ğŸ“ SAVED TO: {drive_output}\n",
    "   â”œâ”€â”€ clean_entry.csv       (Ready for ML!)\n",
    "   â”œâ”€â”€ flagged.csv           (For further cleaning)\n",
    "   â””â”€â”€ logs/\n",
    "       â”œâ”€â”€ cleaning_summary.csv\n",
    "       â””â”€â”€ flag_reasons.csv\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee657653",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”„ PHASE 2: FULL CLEANING PIPELINE\n",
    "================================================================================\n",
    "\n",
    "This phase performs a comprehensive 5-stage data cleaning process on the initial results.\n",
    "\n",
    "**STAGES:**\n",
    "1. Load and assess initial data (clean_entry.csv + flagged.csv)\n",
    "2. Analyze flagged data quality issues\n",
    "3. Clean flagged data â†’ cleaned2.0.csv\n",
    "4. Extract completely clean data â†’ cleaned3.0.csv\n",
    "5. Merge all clean data â†’ final_clean.csv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0a1e0",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“‚ SECTION 12: Load Initial Cleaned Data for Full Cleaning\n",
    "---\n",
    "Load clean_entry.csv and flagged.csv from the initial cleaning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed92ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Full Cleaning\n",
    "INPUT_FILES = {\n",
    "    'clean_entry': '/content/cleaned_data/clean_entry.csv',\n",
    "    'flagged': '/content/flagged_data/flagged.csv'\n",
    "}\n",
    "\n",
    "OUTPUT_FILES = {\n",
    "    'cleaned2': '/content/cleaned_data/cleaned2.0.csv',\n",
    "    'cleaned3': '/content/cleaned_data/cleaned3.0.csv',\n",
    "    'final': '/content/cleaned_data/final_clean.csv',\n",
    "    'report': '/content/logs/cleaning_report.txt'\n",
    "}\n",
    "\n",
    "KEY_COLUMNS = [\n",
    "    'Patient Gender', \n",
    "    'Patient ID', \n",
    "    'Image Index', \n",
    "    'Patient Age', \n",
    "    'diseases', \n",
    "    'OriginalImage[Width', \n",
    "    'OriginalImage[Height', \n",
    "    'View Position', \n",
    "    'Random_Code'\n",
    "]\n",
    "\n",
    "# Age validation parameters\n",
    "MIN_VALID_AGE = 0\n",
    "MAX_VALID_AGE = 120\n",
    "UNREALISTIC_AGES = [999, 150]\n",
    "\n",
    "print(\"âœ… Configuration set for Full Cleaning Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for formatted output\n",
    "def print_stage(stage_num, title):\n",
    "    \"\"\"Print formatted stage header\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"STAGE {stage_num}: {title}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# STAGE 1: Load and Assess Initial Data\n",
    "print_stage(1, \"LOAD AND ASSESS INITIAL DATA\")\n",
    "\n",
    "# Load clean_entry.csv\n",
    "print_section(\"Loading clean_entry.csv\")\n",
    "df_clean_entry = pd.read_csv(INPUT_FILES['clean_entry'])\n",
    "print(f\"âœ… Loaded: {len(df_clean_entry):,} rows\")\n",
    "print(f\"   Columns: {len(df_clean_entry.columns)}\")\n",
    "print(f\"   Unique images: {df_clean_entry['Image Index'].nunique():,}\")\n",
    "print(f\"   Unique patients: {df_clean_entry['Patient ID'].nunique():,}\")\n",
    "\n",
    "# Load flagged.csv\n",
    "print_section(\"Loading flagged.csv\")\n",
    "df_flagged = pd.read_csv(INPUT_FILES['flagged'])\n",
    "print(f\"âœ… Loaded: {len(df_flagged):,} rows\")\n",
    "print(f\"   Columns: {len(df_flagged.columns)}\")\n",
    "\n",
    "# Analyze flag reasons\n",
    "if 'flag_reason' in df_flagged.columns:\n",
    "    print(\"\\nğŸ“‹ Flag reasons breakdown:\")\n",
    "    for reason, count in df_flagged['flag_reason'].value_counts().items():\n",
    "        print(f\"   - {reason}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987ddbc",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ” SECTION 13: Analyze Flagged Data Quality Issues\n",
    "---\n",
    "Analyze gender values, age issues, and missing data in flagged.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2: Analyze Flagged Data Quality Issues\n",
    "print_stage(2, \"ANALYZE FLAGGED DATA QUALITY ISSUES\")\n",
    "\n",
    "# Gender analysis\n",
    "print_section(\"Gender Value Analysis\")\n",
    "print(\"ğŸ“‹ Unique gender values:\")\n",
    "for gender, count in df_flagged['Patient Gender'].value_counts(dropna=False).items():\n",
    "    print(f\"   - '{gender}': {count:,} rows\")\n",
    "\n",
    "# Age analysis\n",
    "print_section(\"Age Value Analysis\")\n",
    "print(\"ğŸ“‹ Sample of problematic age values:\")\n",
    "age_counts = df_flagged['Patient Age'].value_counts().head(20)\n",
    "for age, count in age_counts.items():\n",
    "    print(f\"   - '{age}': {count:,} rows\")\n",
    "\n",
    "# Missing data analysis\n",
    "print_section(\"Missing Data Analysis\")\n",
    "print(\"ğŸ“‹ NaN counts in key columns:\")\n",
    "for col in KEY_COLUMNS:\n",
    "    if col in df_flagged.columns:\n",
    "        nan_count = df_flagged[col].isna().sum()\n",
    "        pct = (nan_count / len(df_flagged)) * 100\n",
    "        print(f\"   - {col:30s}: {nan_count:5,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a0d20",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ§¼ SECTION 14: Clean Gender Column\n",
    "---\n",
    "Standardize gender values: 'female'/'f' â†’ 'F', 'male'/'m' â†’ 'M', all others â†’ NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a91f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gender(gender):\n",
    "    \"\"\"\n",
    "    Standardize gender values to F, M, or NaN\n",
    "    \n",
    "    Args:\n",
    "        gender: Original gender value\n",
    "        \n",
    "    Returns:\n",
    "        Standardized gender ('F', 'M', or NaN)\n",
    "    \"\"\"\n",
    "    if pd.isna(gender):\n",
    "        return np.nan\n",
    "    \n",
    "    gender_str = str(gender).strip().lower()\n",
    "    \n",
    "    # Map to standard F/M\n",
    "    if gender_str in ['f', 'female']:\n",
    "        return 'F'\n",
    "    elif gender_str in ['m', 'male']:\n",
    "        return 'M'\n",
    "    else:\n",
    "        # Keep unknown, N, or other invalid values as NaN\n",
    "        return np.nan\n",
    "\n",
    "# STAGE 3 (Part 1): Clean Gender\n",
    "print_stage(3, \"CLEAN FLAGGED DATA â†’ cleaned2.0.csv\")\n",
    "\n",
    "df_cleaned2 = df_flagged.copy()\n",
    "initial_rows = len(df_cleaned2)\n",
    "\n",
    "print_section(\"Cleaning Gender Column\")\n",
    "original_valid_gender = df_cleaned2['Patient Gender'].notna().sum()\n",
    "df_cleaned2['Patient Gender'] = df_cleaned2['Patient Gender'].apply(clean_gender)\n",
    "cleaned_valid_gender = df_cleaned2['Patient Gender'].notna().sum()\n",
    "\n",
    "print(f\"  Before: {original_valid_gender:,} valid gender values\")\n",
    "print(f\"  After:  {cleaned_valid_gender:,} valid gender values (F/M only)\")\n",
    "print(f\"  Improvement: +{cleaned_valid_gender - original_valid_gender:,} standardized values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9c9b2",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”¢ SECTION 15: Clean Age Column\n",
    "---\n",
    "Convert and validate ages: handle text ages, round decimals, filter unrealistic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbde1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_age(age):\n",
    "    \"\"\"\n",
    "    Convert and validate age values\n",
    "    \n",
    "    Args:\n",
    "        age: Original age value\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned age (integer 0-120, or NaN if invalid)\n",
    "    \"\"\"\n",
    "    if pd.isna(age):\n",
    "        return np.nan\n",
    "    \n",
    "    # If already numeric and valid, return as float\n",
    "    try:\n",
    "        age_num = float(age)\n",
    "        # Filter out unrealistic ages\n",
    "        if age_num < MIN_VALID_AGE or age_num > MAX_VALID_AGE or age_num in UNREALISTIC_AGES:\n",
    "            return np.nan\n",
    "        # Handle decimal ages - round to nearest integer\n",
    "        return round(age_num)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Handle text ages\n",
    "    age_str = str(age).strip().lower()\n",
    "    \n",
    "    # Map common text ages\n",
    "    text_age_map = {\n",
    "        'twenty five': 25,\n",
    "        'unknown': np.nan,\n",
    "    }\n",
    "    \n",
    "    if age_str in text_age_map:\n",
    "        return text_age_map[age_str]\n",
    "    \n",
    "    # If we can't convert, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# STAGE 3 (Part 2): Clean Age\n",
    "print_section(\"Cleaning Age Column\")\n",
    "original_valid_age = df_cleaned2['Patient Age'].notna().sum()\n",
    "df_cleaned2['Patient Age'] = df_cleaned2['Patient Age'].apply(clean_age)\n",
    "cleaned_valid_age = df_cleaned2['Patient Age'].notna().sum()\n",
    "\n",
    "print(f\"  Before: {original_valid_age:,} non-null age values\")\n",
    "print(f\"  After:  {cleaned_valid_age:,} valid age values (0-120)\")\n",
    "\n",
    "if cleaned_valid_age > 0:\n",
    "    print(f\"  Age range: {df_cleaned2['Patient Age'].min():.0f} to {df_cleaned2['Patient Age'].max():.0f}\")\n",
    "    print(f\"  Mean age: {df_cleaned2['Patient Age'].mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8322837",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ’¾ SECTION 16: Save Cleaned Flagged Data (cleaned2.0.csv)\n",
    "---\n",
    "Save the cleaned flagged data with standardized gender and age values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 3 (Part 3): Save cleaned2.0.csv\n",
    "print_section(\"Cleaning Summary\")\n",
    "print(f\"  Total rows retained: {len(df_cleaned2):,} (100% - no rows removed)\")\n",
    "print(f\"  Gender standardized: {df_cleaned2['Patient Gender'].notna().sum():,} valid (F/M)\")\n",
    "print(f\"  Age cleaned: {df_cleaned2['Patient Age'].notna().sum():,} valid (0-120)\")\n",
    "\n",
    "# Save cleaned2.0.csv\n",
    "df_cleaned2.to_csv(OUTPUT_FILES['cleaned2'], index=False)\n",
    "print(f\"\\nâœ… Saved: {OUTPUT_FILES['cleaned2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1117c",
   "metadata": {},
   "source": [
    "---\n",
    "# âœ¨ SECTION 17: Extract Completely Clean Data (cleaned3.0.csv)\n",
    "---\n",
    "Filter to keep only rows with NO NaN values in key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 4: Extract Completely Clean Data\n",
    "print_stage(4, \"EXTRACT COMPLETELY CLEAN DATA â†’ cleaned3.0.csv\")\n",
    "\n",
    "# Check NaN counts\n",
    "print_section(\"NaN Analysis in Key Columns\")\n",
    "for col in KEY_COLUMNS:\n",
    "    if col in df_cleaned2.columns:\n",
    "        nan_count = df_cleaned2[col].isna().sum()\n",
    "        pct = (nan_count / len(df_cleaned2)) * 100\n",
    "        print(f\"  {col:30s}: {nan_count:5,} NaN ({pct:5.1f}%)\")\n",
    "\n",
    "# Create filter mask for completely clean data\n",
    "print_section(\"Filtering for Complete Data\")\n",
    "\n",
    "# Check which key columns exist\n",
    "existing_key_cols = [col for col in KEY_COLUMNS if col in df_cleaned2.columns]\n",
    "clean_mask = df_cleaned2[existing_key_cols].notna().all(axis=1)\n",
    "\n",
    "# Also check that diseases is not empty string\n",
    "if 'diseases' in df_cleaned2.columns:\n",
    "    clean_mask &= df_cleaned2['diseases'].str.strip() != ''\n",
    "\n",
    "df_cleaned3 = df_cleaned2[clean_mask].copy()\n",
    "\n",
    "rows_removed = len(df_cleaned2) - len(df_cleaned3)\n",
    "pct_clean = (len(df_cleaned3) / len(df_cleaned2)) * 100 if len(df_cleaned2) > 0 else 0\n",
    "\n",
    "print(f\"  Total rows: {len(df_cleaned2):,}\")\n",
    "print(f\"  Complete rows: {len(df_cleaned3):,}\")\n",
    "print(f\"  Rows removed: {rows_removed:,}\")\n",
    "print(f\"  Clean data percentage: {pct_clean:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display clean data statistics\n",
    "print_section(\"Clean Data Statistics\")\n",
    "\n",
    "if len(df_cleaned3) > 0:\n",
    "    print(f\"ğŸ“Š Gender distribution:\")\n",
    "    for gender, count in df_cleaned3['Patient Gender'].value_counts().items():\n",
    "        print(f\"   - {gender}: {count:,}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Age statistics:\")\n",
    "    print(f\"   - Count: {len(df_cleaned3):,}\")\n",
    "    print(f\"   - Min: {df_cleaned3['Patient Age'].min():.0f}\")\n",
    "    print(f\"   - Max: {df_cleaned3['Patient Age'].max():.0f}\")\n",
    "    print(f\"   - Mean: {df_cleaned3['Patient Age'].mean():.1f}\")\n",
    "    \n",
    "    if 'View Position' in df_cleaned3.columns:\n",
    "        print(f\"\\nğŸ“Š View Position distribution:\")\n",
    "        for view, count in df_cleaned3['View Position'].value_counts().items():\n",
    "            print(f\"   - {view}: {count:,}\")\n",
    "    \n",
    "    if 'diseases' in df_cleaned3.columns:\n",
    "        print(f\"\\nğŸ“Š Top 5 diseases:\")\n",
    "        for disease, count in df_cleaned3['diseases'].value_counts().head(5).items():\n",
    "            print(f\"   - {disease}: {count:,}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No completely clean rows found after filtering\")\n",
    "\n",
    "# Save cleaned3.0.csv\n",
    "df_cleaned3.to_csv(OUTPUT_FILES['cleaned3'], index=False)\n",
    "print(f\"\\nâœ… Saved: {OUTPUT_FILES['cleaned3']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cab0a0",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”€ SECTION 18: Merge All Clean Data (final_clean.csv)\n",
    "---\n",
    "Combine clean_entry.csv and cleaned3.0.csv, remove duplicates, create final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 5: Merge All Clean Data\n",
    "print_stage(5, \"MERGE ALL CLEAN DATA â†’ final_clean.csv\")\n",
    "\n",
    "# Check column compatibility\n",
    "print_section(\"Checking Column Compatibility\")\n",
    "cols1 = set(df_clean_entry.columns)\n",
    "cols2 = set(df_cleaned3.columns)\n",
    "\n",
    "print(f\"  clean_entry.csv columns: {len(cols1)}\")\n",
    "print(f\"  cleaned3.0.csv columns: {len(cols2)}\")\n",
    "\n",
    "# Align columns if different\n",
    "if cols1 != cols2:\n",
    "    print(\"\\n  Column differences found - aligning...\")\n",
    "    only_in_clean_entry = cols1 - cols2\n",
    "    only_in_cleaned3 = cols2 - cols1\n",
    "    \n",
    "    if only_in_clean_entry:\n",
    "        print(f\"    Only in clean_entry: {only_in_clean_entry}\")\n",
    "    if only_in_cleaned3:\n",
    "        print(f\"    Only in cleaned3.0: {only_in_cleaned3}\")\n",
    "    \n",
    "    # Add missing columns\n",
    "    all_cols = sorted(cols1.union(cols2))\n",
    "    for col in all_cols:\n",
    "        if col not in df_clean_entry.columns:\n",
    "            df_clean_entry[col] = None\n",
    "        if col not in df_cleaned3.columns:\n",
    "            df_cleaned3[col] = None\n",
    "    \n",
    "    # Reorder columns\n",
    "    df_clean_entry = df_clean_entry[all_cols]\n",
    "    df_cleaned3 = df_cleaned3[all_cols]\n",
    "    print(\"    âœ… Columns aligned!\")\n",
    "\n",
    "# Combine datasets\n",
    "print_section(\"Combining Datasets\")\n",
    "print(f\"  clean_entry.csv: {len(df_clean_entry):,} rows\")\n",
    "print(f\"  cleaned3.0.csv:  {len(df_cleaned3):,} rows\")\n",
    "\n",
    "df_final = pd.concat([df_clean_entry, df_cleaned3], ignore_index=True)\n",
    "print(f\"  Total after concat: {len(df_final):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfedb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print_section(\"Removing Duplicates\")\n",
    "initial_count = len(df_final)\n",
    "df_final = df_final.drop_duplicates(subset=['Image Index'], keep='first')\n",
    "duplicates_removed = initial_count - len(df_final)\n",
    "\n",
    "print(f\"  Before deduplication: {initial_count:,} rows\")\n",
    "print(f\"  After deduplication:  {len(df_final):,} rows\")\n",
    "print(f\"  Duplicates removed:   {duplicates_removed:,} rows\")\n",
    "\n",
    "# Verify uniqueness\n",
    "is_unique = df_final['Image Index'].nunique() == len(df_final)\n",
    "print(f\"  All images unique: {'âœ… YES' if is_unique else 'âŒ NO'}\")\n",
    "\n",
    "# Final statistics\n",
    "print_section(\"Final Dataset Statistics\")\n",
    "print(f\"  Total rows: {len(df_final):,}\")\n",
    "print(f\"  Unique images: {df_final['Image Index'].nunique():,}\")\n",
    "print(f\"  Unique patients: {df_final['Patient ID'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Gender distribution:\")\n",
    "for gender, count in df_final['Patient Gender'].value_counts(dropna=False).items():\n",
    "    print(f\"   - {gender}: {count:,}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Age statistics:\")\n",
    "valid_ages = df_final['Patient Age'].notna().sum()\n",
    "print(f\"   - Valid ages: {valid_ages:,}\")\n",
    "print(f\"   - Missing ages: {df_final['Patient Age'].isna().sum():,}\")\n",
    "if valid_ages > 0:\n",
    "    print(f\"   - Min: {df_final['Patient Age'].min():.0f}\")\n",
    "    print(f\"   - Max: {df_final['Patient Age'].max():.0f}\")\n",
    "    print(f\"   - Mean: {df_final['Patient Age'].mean():.1f}\")\n",
    "\n",
    "if 'View Position' in df_final.columns:\n",
    "    print(f\"\\nğŸ“Š View Position distribution:\")\n",
    "    for view, count in df_final['View Position'].value_counts(dropna=False).items():\n",
    "        print(f\"   - {view}: {count:,}\")\n",
    "\n",
    "# Determine diseases column name\n",
    "diseases_col = 'diseases' if 'diseases' in df_final.columns else 'Finding Labels'\n",
    "if diseases_col in df_final.columns:\n",
    "    print(f\"\\nğŸ“Š Top 10 diseases:\")\n",
    "    for disease, count in df_final[diseases_col].value_counts().head(10).items():\n",
    "        print(f\"   - {disease}: {count:,}\")\n",
    "\n",
    "# Save final_clean.csv\n",
    "df_final.to_csv(OUTPUT_FILES['final'], index=False)\n",
    "print(f\"\\nâœ… Saved: {OUTPUT_FILES['final']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357dbb8",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ SECTION 19: Generate Final Cleaning Report\n",
    "---\n",
    "Generate comprehensive cleaning_report.txt with statistics for all stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Cleaning Report\n",
    "print_stage(\"FINAL\", \"GENERATING CLEANING REPORT\")\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append(\"=\" * 80)\n",
    "report_lines.append(\"MEDICAL IMAGING DATASET - COMPLETE CLEANING REPORT\")\n",
    "report_lines.append(\"=\" * 80)\n",
    "report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Stage 1: Input Data\n",
    "report_lines.append(\"STAGE 1: INPUT DATA\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"clean_entry.csv:  {len(df_clean_entry):,} rows (pre-cleaned data)\")\n",
    "report_lines.append(f\"flagged.csv:      {len(df_flagged):,} rows (data with issues)\")\n",
    "report_lines.append(f\"Total input:      {len(df_clean_entry) + len(df_flagged):,} rows\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Stage 2: Flag Analysis\n",
    "report_lines.append(\"STAGE 2: FLAGGED DATA ISSUES\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "if 'flag_reason' in df_flagged.columns:\n",
    "    for reason, count in df_flagged['flag_reason'].value_counts().items():\n",
    "        report_lines.append(f\"  {reason}: {count:,} rows\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Stage 3: Cleaned Flagged Data\n",
    "report_lines.append(\"STAGE 3: CLEANED FLAGGED DATA (cleaned2.0.csv)\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Total rows: {len(df_cleaned2):,} (100% retained)\")\n",
    "report_lines.append(f\"Valid gender (F/M): {df_cleaned2['Patient Gender'].notna().sum():,}\")\n",
    "report_lines.append(f\"Valid age (0-120): {df_cleaned2['Patient Age'].notna().sum():,}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Stage 4: Completely Clean Data\n",
    "report_lines.append(\"STAGE 4: COMPLETELY CLEAN DATA (cleaned3.0.csv)\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Total rows: {len(df_cleaned3):,}\")\n",
    "if len(df_flagged) > 0:\n",
    "    report_lines.append(f\"Percentage of flagged data: {len(df_cleaned3)/len(df_flagged)*100:.1f}%\")\n",
    "report_lines.append(f\"All key columns: 100% complete (no NaN)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Stage 5: Final Merged Data\n",
    "report_lines.append(\"STAGE 5: FINAL MERGED DATA (final_clean.csv)\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Total unique rows: {len(df_final):,}\")\n",
    "report_lines.append(f\"Unique images: {df_final['Image Index'].nunique():,}\")\n",
    "report_lines.append(f\"Unique patients: {df_final['Patient ID'].nunique():,}\")\n",
    "report_lines.append(f\"Duplicates removed: {(len(df_clean_entry) + len(df_cleaned3)) - len(df_final):,}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Final Quality Metrics\n",
    "report_lines.append(\"FINAL QUALITY METRICS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Gender completeness: {df_final['Patient Gender'].notna().sum()/len(df_final)*100:.1f}%\")\n",
    "report_lines.append(f\"Age completeness: {df_final['Patient Age'].notna().sum()/len(df_final)*100:.1f}%\")\n",
    "\n",
    "diseases_col = 'diseases' if 'diseases' in df_final.columns else 'Finding Labels'\n",
    "if diseases_col in df_final.columns:\n",
    "    report_lines.append(f\"Disease labels: {df_final[diseases_col].notna().sum()/len(df_final)*100:.1f}%\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Gender Distribution:\")\n",
    "for gender, count in df_final['Patient Gender'].value_counts(dropna=False).items():\n",
    "    pct = count / len(df_final) * 100\n",
    "    report_lines.append(f\"  {gender}: {count:,} ({pct:.1f}%)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "valid_ages = df_final['Patient Age'].notna().sum()\n",
    "if valid_ages > 0:\n",
    "    report_lines.append(\"Age Statistics:\")\n",
    "    report_lines.append(f\"  Valid: {valid_ages:,}\")\n",
    "    report_lines.append(f\"  Min: {df_final['Patient Age'].min():.0f}\")\n",
    "    report_lines.append(f\"  Max: {df_final['Patient Age'].max():.0f}\")\n",
    "    report_lines.append(f\"  Mean: {df_final['Patient Age'].mean():.1f}\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "if diseases_col in df_final.columns:\n",
    "    report_lines.append(\"Top 10 Diseases:\")\n",
    "    for i, (disease, count) in enumerate(df_final[diseases_col].value_counts().head(10).items(), 1):\n",
    "        pct = count / len(df_final) * 100\n",
    "        report_lines.append(f\"  {i:2d}. {disease}: {count:,} ({pct:.1f}%)\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "# Output files\n",
    "report_lines.append(\"OUTPUT FILES GENERATED\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "for key, filename in OUTPUT_FILES.items():\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
    "        report_lines.append(f\"  âœ“ {filename} ({size:.2f} MB)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"=\" * 80)\n",
    "report_lines.append(\"CLEANING COMPLETE - DATASET READY FOR ANALYSIS!\")\n",
    "report_lines.append(\"=\" * 80)\n",
    "\n",
    "# Write report\n",
    "report_text = \"\\n\".join(report_lines)\n",
    "with open(OUTPUT_FILES['report'], 'w') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(report_text)\n",
    "print(f\"\\nâœ… Report saved: {OUTPUT_FILES['report']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5360c",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ’¾ SECTION 20: Save All Files to Google Drive\n",
    "---\n",
    "Copy all output files to Google Drive and display final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all files to Google Drive\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ’¾ SAVING ALL FILES TO GOOGLE DRIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Copy cleaned files to Drive\n",
    "shutil.copy(OUTPUT_FILES['cleaned2'], f'{drive_output}/cleaned2.0.csv')\n",
    "print(f\"âœ… Copied: cleaned2.0.csv\")\n",
    "\n",
    "shutil.copy(OUTPUT_FILES['cleaned3'], f'{drive_output}/cleaned3.0.csv')\n",
    "print(f\"âœ… Copied: cleaned3.0.csv\")\n",
    "\n",
    "shutil.copy(OUTPUT_FILES['final'], f'{drive_output}/final_clean.csv')\n",
    "print(f\"âœ… Copied: final_clean.csv\")\n",
    "\n",
    "shutil.copy(OUTPUT_FILES['report'], f'{drive_output}/logs/cleaning_report.txt')\n",
    "print(f\"âœ… Copied: cleaning_report.txt\")\n",
    "\n",
    "print(f\"\\nğŸ“ All files saved to: {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23234c",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ‰ PIPELINE COMPLETE!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ COMPLETE DATA CLEANING PIPELINE - SUCCESS!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Calculate file sizes\n",
    "def get_file_size_mb(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        return os.path.getsize(filepath) / (1024 * 1024)\n",
    "    return 0\n",
    "\n",
    "diseases_col = 'diseases' if 'diseases' in df_final.columns else 'Finding Labels'\n",
    "\n",
    "print(f\"\"\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ“Š FINAL DATASET STATISTICS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ¯ FINAL_CLEAN.CSV\n",
    "   Total records:     {len(df_final):,}\n",
    "   Unique images:     {df_final['Image Index'].nunique():,}\n",
    "   Unique patients:   {df_final['Patient ID'].nunique():,}\n",
    "   \n",
    "ğŸ“ˆ DATA QUALITY\n",
    "   Gender complete:   {df_final['Patient Gender'].notna().sum()/len(df_final)*100:.1f}%\n",
    "   Age complete:      {df_final['Patient Age'].notna().sum()/len(df_final)*100:.1f}%\n",
    "   Disease labels:    {df_final[diseases_col].notna().sum()/len(df_final)*100:.1f}%\n",
    "\n",
    "ğŸ“ OUTPUT FILES\n",
    "   {drive_output}/\n",
    "   â”œâ”€â”€ clean_entry.csv      ({get_file_size_mb(INPUT_FILES['clean_entry']):.2f} MB)\n",
    "   â”œâ”€â”€ flagged.csv          ({get_file_size_mb(INPUT_FILES['flagged']):.2f} MB)\n",
    "   â”œâ”€â”€ cleaned2.0.csv       ({get_file_size_mb(OUTPUT_FILES['cleaned2']):.2f} MB)\n",
    "   â”œâ”€â”€ cleaned3.0.csv       ({get_file_size_mb(OUTPUT_FILES['cleaned3']):.2f} MB)\n",
    "   â”œâ”€â”€ final_clean.csv      ({get_file_size_mb(OUTPUT_FILES['final']):.2f} MB) â­ READY FOR ML!\n",
    "   â””â”€â”€ logs/\n",
    "       â”œâ”€â”€ cleaning_summary.csv\n",
    "       â”œâ”€â”€ flag_reasons.csv\n",
    "       â””â”€â”€ cleaning_report.txt\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ… Dataset is now ready for Machine Learning and Analysis!\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
